{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "colab_type": "text",
    "id": "5ymw4CB2EH4Q"
   },
   "source": [
    "this notebook prepare posts and events for training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 31366,
     "status": "ok",
     "timestamp": 1586262978862,
     "user": {
      "displayName": "Михаил Ковальчук",
      "photoUrl": "",
      "userId": "00328138544128199672"
     },
     "user_tz": -180
    },
    "id": "f_gq-hpeDHu4",
    "outputId": "f6f95c88-38a9-4c6f-b2d0-f58ad3cb0247",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "import gensim.models\n",
    "\n",
    "DATA_PATH = '/data/'\n",
    "CAPTION_PATH = DATA_PATH + 'captions/'\n",
    "RAW_CAPTION_PATH = CAPTION_PATH + 'raw/'\n",
    "\n",
    "ClEAN_CAPTION_PATH = CAPTION_PATH + 'cleaned/'\n",
    "LEM_CAPTION_PATH = CAPTION_PATH + 'lem/'\n",
    "\n",
    "cities = ['spb', 'moscow']#, 'nyc', 'london']\n",
    "years = ['2017', '2018', '2019', '2020']\n",
    "files = []\n",
    "for city in cities:\n",
    "    for year in years:\n",
    "        files.append([city, year])\n",
    "\n",
    "def csv_path(path, city, year):\n",
    "    return path + city + '_posts_' + year + '.csv'\n",
    "\n",
    "\n",
    "import fasttext\n",
    "\n",
    "PRETRAINED_MODEL_PATH = '/data/source/lid.176.bin'\n",
    "lang = fasttext.load_model(PRETRAINED_MODEL_PATH) # model for defining a language of text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell \n",
    "    1. replceces '\\n', tab symbols and other space symbols to simple space\n",
    "    2. removes duplicated spaces \n",
    "    3. removes all symbols, which aren't a letter or space (exception - numbers in a word - '8марта')\n",
    "    4. defines language of posts\n",
    "    5. removes posts without words\n",
    "    6. label language of post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gec_EjMzEXcg",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished spb 2017\n",
      "finished spb 2018\n",
      "finished spb 2019\n",
      "finished spb 2020\n",
      "finished spb; time on stage: 3446.691294670105; counted posts: 16632760\n",
      "finished moscow 2017\n",
      "finished moscow 2018\n",
      "finished moscow 2019\n",
      "finished moscow 2020\n",
      "finished moscow; time on stage: 4696.5913808345795; counted posts: 19327608\n",
      "completed with time: 8143.283170223236\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'8441'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.parsing.preprocessing import strip_multiple_whitespaces\n",
    "MIN_DOCUMENT_SIZE = 1\n",
    "trans_rules = str.maketrans({ \n",
    "    \"\\n\": \" \", \"\\r\": \" \", \"\\xa0\": \" \",\n",
    "    \"#\": \" #\", #\"@\": \" @\",\n",
    "    \"_\": \" \"\n",
    "})\n",
    "\n",
    "def is_allowed_chars(char: str):\n",
    "    return char.isalpha() or char.isdigit() or char.isspace() or char in set(['#'])#, '@'])\n",
    "\n",
    "def is_allowed_word(word: str):\n",
    "    return len(word) > 0 and not word.isdigit()\n",
    "\n",
    "def preprocessor(s: str):\n",
    "    # all letter to lower case\n",
    "    s = s.lower() \n",
    "    s = s.translate(trans_rules)\n",
    "    # remove all symbol, which aren't letter or num\n",
    "    s = ''.join([char for char in s if is_allowed_chars(char)])\n",
    "    # replace all space symbol like space, tab, \\n to simple space and remove double and for space\n",
    "    s = strip_multiple_whitespaces(s)\n",
    "    # remove empty words and words from filtered words \n",
    "    s = ' '.join([word for word in s.split(' ') if is_allowed_word(word)])\n",
    "    return s\n",
    "\n",
    "\n",
    "cities_counted_posts = dict([(city, 0) for city in cities])\n",
    "old_time = time.time()\n",
    "start_time = old_time\n",
    "\n",
    "for city in cities:\n",
    "    for year in years:\n",
    "        # load file and drop documents with empty text\n",
    "        df = pd.read_csv(csv_path(RAW_CAPTION_PATH, city, year)).dropna()\n",
    "        # preprocess of corpus\n",
    "        df['caption'] = df['caption'].apply(preprocessor)\n",
    "        # remove empty documents\n",
    "        df = df[df['caption'].map(lambda t: len(t.split())) >= MIN_DOCUMENT_SIZE]\n",
    "        \n",
    "        languages = []\n",
    "        for s in df['caption']:\n",
    "            languages.append(lang.predict(s)[0][0])\n",
    "        df['lang'] = languages\n",
    "        \n",
    "        cities_counted_posts[city] += len(df)\n",
    "        \n",
    "        # save results\n",
    "        df.to_csv(r'' + csv_path(ClEAN_CAPTION_PATH, city, year), index=False)\n",
    "        print(f\"finished {city} {year}\")\n",
    "        \n",
    "    print(f'finished {city}; time on stage: {time.time() - old_time}; counted posts: {cities_counted_posts[city]}')\n",
    "    old_time = time.time()\n",
    "\n",
    "print(f'completed with time: {time.time() - start_time}')\n",
    "'8441'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ClNISbTos-PL"
   },
   "source": [
    "Вместо леммитицазии всех докуентов поотдельности можно обработать весь текст, после составить словарь всех слов, леммитизировать все по одному разу и перезаписать текст постов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "algorithm of lemmatize using dictionary of all words in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded words from spb 2017\n",
      "loaded words from spb 2018\n",
      "loaded words from spb 2019\n",
      "loaded words from spb 2020\n",
      "loaded words from moscow 2017\n",
      "loaded words from moscow 2018\n",
      "loaded words from moscow 2019\n",
      "loaded words from moscow 2020\n",
      "time on stage: 1694.864534854889\n",
      "finish of lemmatize; time on stage: 689.824747800827\n",
      "replaced words for spb 2017\n",
      "replaced words for spb 2018\n",
      "replaced words for spb 2019\n",
      "replaced words for spb 2020\n",
      "replaced words for moscow 2017\n",
      "replaced words for moscow 2018\n",
      "replaced words for moscow 2019\n",
      "replaced words for moscow 2020\n",
      "finish; time on stage: 1827.7670729160309; all time: 4212.4567086696625\n"
     ]
    }
   ],
   "source": [
    "# fast lemmatize\n",
    "from collections import defaultdict, Counter\n",
    "import time\n",
    "from pymystem3 import Mystem # for normalization of text\n",
    "my_stem = Mystem()\n",
    "\n",
    "start_time = time.time()\n",
    "old_time = time.time()\n",
    "\n",
    "global_words = defaultdict(int)\n",
    "global_hashtags = defaultdict(int)\n",
    "idf = defaultdict(int)\n",
    "idf_lemmatized = defaultdict(int)\n",
    "words_count = 0\n",
    "valid_langs = set(['__label__ru', '__label__en'])\n",
    "\n",
    "\n",
    "# load all words to dictionary\n",
    "for city, year in files:\n",
    "    df = pd.read_csv(csv_path(ClEAN_CAPTION_PATH, city, year)).dropna()\n",
    "    df = df[df.lang.isin(valid_langs)]\n",
    "    for caption in df['caption']:\n",
    "        words = caption.split(' ')\n",
    "        words_count += len(words)\n",
    "        counts = dict(Counter(words))\n",
    "        for word in counts:\n",
    "            if word[0] in ['#', '@']:\n",
    "                global_hashtags[word] += counts[word]\n",
    "            else:\n",
    "                global_words[word] += counts[word]\n",
    "            idf[word] += 1\n",
    "    \n",
    "    print(f'loaded words from {city} {year}')\n",
    "\n",
    "print(f'time on stage: {time.time() - old_time}')\n",
    "old_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "# create lemmatized dictionary\n",
    "word_to_lem = dict([(word, ''.join(my_stem.lemmatize(word)[:-1])) for word in global_words]) # [:-1] for remove \\n after lemmatize\n",
    "print(f'finish of lemmatize; time on stage: {time.time() - old_time}')\n",
    "old_time = time.time()\n",
    "\n",
    "for hashtag in global_hashtags:\n",
    "    word_to_lem[hashtag] = hashtag\n",
    "\n",
    "def lemmatize_caption(caption: str):\n",
    "    return ' '.join([word_to_lem[word] for word in caption.split() if word in word_to_lem])\n",
    "\n",
    "posts_count = 0\n",
    "# replece all words in dataset to lemmatized words\n",
    "for city, year in files:\n",
    "    df = pd.read_csv(csv_path(ClEAN_CAPTION_PATH, city, year)).dropna()\n",
    "    # df = df[df.lang.isin(valid_langs)]\n",
    "    df['caption'] = df['caption'].apply(lemmatize_caption)\n",
    "    df = df.dropna()\n",
    "    posts_count += len(df)\n",
    "    for caption in df['caption']:\n",
    "        for word in set(caption.split(' ')):\n",
    "            idf_lemmatized[word] += 1\n",
    "    df.to_csv(r'' + csv_path(LEM_CAPTION_PATH, city, year), index=False)\n",
    "    print(f'replaced words for {city} {year}')\n",
    "\n",
    "\n",
    "print(f'finish; time on stage: {time.time() - old_time}; all time: {time.time() - start_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dictionary \n",
    "import math\n",
    "\n",
    "global_words.update(global_hashtags)\n",
    "\n",
    "global_lems = defaultdict(int)\n",
    "for word in global_words:\n",
    "    global_lems[word_to_lem[word]] += global_words[word]\n",
    "    \n",
    "columns = ['word', 'idf', 'count_usages', 'lemmatized', 'idf_lem', 'count_usages_lem']\n",
    "\n",
    "def make_row(word: str):\n",
    "    lem = word_to_lem[word]\n",
    "    word_idf = math.log(posts_count / idf[word])\n",
    "    lem_idf = math.log(posts_count / idf_lemmatized[lem])\n",
    "    return (word, word_idf, global_words[word], lem, lem_idf, global_lems[lem])\n",
    "\n",
    "rows = [make_row(word) for word in global_words]\n",
    "\n",
    "df = pd.DataFrame(rows, columns=columns)\n",
    "df_lem = pd.DataFrame(zip(*list(zip(*rows))[3:]), columns=columns[3:]).drop_duplicates()\n",
    "df.to_csv(r'' + CAPTION_PATH + 'dict.csv', index=False)\n",
    "df_lem.to_csv(r'' + CAPTION_PATH + 'dict_lem.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gfhj'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key = 'gfhj'\n",
    "word = key if not key[0] in ['#', '@'] else key[1:]\n",
    "word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded words from spb 2017\n",
      "loaded words from spb 2018\n",
      "loaded words from spb 2019\n",
      "loaded words from spb 2020\n",
      "loaded words from moscow 2017\n",
      "loaded words from moscow 2018\n",
      "loaded words from moscow 2019\n",
      "loaded words from moscow 2020\n",
      "time on stage: 1757.6985099315643\n",
      "finish of lemmatize; time on stage: 1086.2971937656403\n"
     ]
    }
   ],
   "source": [
    "# fast lemmatize\n",
    "from collections import defaultdict, Counter\n",
    "import time\n",
    "from pymystem3 import Mystem # for normalization of text\n",
    "my_stem = Mystem()\n",
    "\n",
    "start_time = time.time()\n",
    "old_time = time.time()\n",
    "\n",
    "global_words = defaultdict(int)\n",
    "idf = defaultdict(int)\n",
    "idf_lemmatized = defaultdict(int)\n",
    "words_count = 0\n",
    "valid_langs = set(['__label__ru', '__label__en'])\n",
    "\n",
    "\n",
    "# load all words to dictionary\n",
    "for city, year in files:\n",
    "    df = pd.read_csv(csv_path(ClEAN_CAPTION_PATH, city, year)).dropna()\n",
    "    df = df[df.lang.isin(valid_langs)]\n",
    "    for caption in df['caption']:\n",
    "        words = caption.split(' ')\n",
    "        words_count += len(words)\n",
    "        counts = dict(Counter(words))\n",
    "        for key in counts:\n",
    "            word = key if not key[0] in ['#', '@'] else key[1:]\n",
    "            global_words[word] += counts[key]\n",
    "            idf[word] += 1\n",
    "    \n",
    "    print(f'loaded words from {city} {year}')\n",
    "\n",
    "print(f'time on stage: {time.time() - old_time}')\n",
    "old_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "# create lemmatized dictionary\n",
    "word_to_lem = dict([(word, ''.join(my_stem.lemmatize(word)[:-1])) for word in global_words]) # [:-1] for remove \\n after lemmatize\n",
    "print(f'finish of lemmatize; time on stage: {time.time() - old_time}')\n",
    "old_time = time.time()\n",
    "\n",
    "def lemmatize_caption(caption: str):\n",
    "    return ' '.join([word_to_lem[word] for word in caption.split() if word in word_to_lem])\n",
    "\n",
    "posts_count = 0\n",
    "# replece all words in dataset to lemmatized words\n",
    "for city, year in files:\n",
    "    df = pd.read_csv(csv_path(ClEAN_CAPTION_PATH, city, year)).dropna()\n",
    "    # df = df[df.lang.isin(valid_langs)]\n",
    "    df['caption'] = df['caption'].apply(lambda s: s.replace('@', '').replace('#', ''))\n",
    "    df['caption'] = df['caption'].apply(lemmatize_caption)\n",
    "    df = df.dropna()\n",
    "    posts_count += len(df)\n",
    "    for caption in df['caption']:\n",
    "        for word in set(caption.split(' ')):\n",
    "            idf_lemmatized[word] += 1\n",
    "\n",
    "\n",
    "import math\n",
    "\n",
    "global_lems = defaultdict(int)\n",
    "for word in global_words:\n",
    "    global_lems[word_to_lem[word]] += global_words[word]\n",
    "    \n",
    "columns = ['word', 'idf', 'count_usages', 'lemmatized', 'idf_lem', 'count_usages_lem']\n",
    "\n",
    "def make_row(word: str):\n",
    "    lem = word_to_lem[word]\n",
    "    word_idf = math.log(posts_count / idf[word])\n",
    "    lem_idf = math.log(posts_count / idf_lemmatized[lem])\n",
    "    return (word, word_idf, global_words[word], lem, lem_idf, global_lems[lem])\n",
    "\n",
    "rows = [make_row(word) for word in global_words]\n",
    "\n",
    "df = pd.DataFrame(rows, columns=columns)\n",
    "df_lem = pd.DataFrame(zip(*list(zip(*rows))[3:]), columns=columns[3:]).drop_duplicates()\n",
    "df.to_csv(r'' + CAPTION_PATH + 'dict_without_hash.csv', index=False)\n",
    "df_lem.to_csv(r'' + CAPTION_PATH + 'dict_lem_without_hash.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'сондляслабак'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_lem['сондляслабаков']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1116"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf_lemmatized['сондляслабак']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_words['сондляслабак']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'сондляслабак'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_lem['сондляслабаков']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMZsRMlgE+83bZ7K7jNXAb5",
   "machine_shape": "hm",
   "name": "filter_caption.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
