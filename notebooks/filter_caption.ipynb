{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5ymw4CB2EH4Q"
   },
   "source": [
    "init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 31366,
     "status": "ok",
     "timestamp": 1586262978862,
     "user": {
      "displayName": "Михаил Ковальчук",
      "photoUrl": "",
      "userId": "00328138544128199672"
     },
     "user_tz": -180
    },
    "id": "f_gq-hpeDHu4",
    "outputId": "f6f95c88-38a9-4c6f-b2d0-f58ad3cb0247",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    }
   ],
   "source": [
    "from pymystem3 import Mystem\n",
    "my_stem = Mystem()\n",
    "\n",
    "import nltk\n",
    "\n",
    "try:\n",
    "    nltk.corpus.stopwords.words('english')\n",
    "except:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "import gensim.models\n",
    "\n",
    "DATA_PATH = '/data/'\n",
    "ROW_CAPTION_PATH = DATA_PATH + 'row_caption/'\n",
    "CAPTION_PATH = DATA_PATH + 'caption/'\n",
    "LEM_CAPTION_PATH = DATA_PATH + 'lem_caption/'\n",
    "CAPTIONS = ['posts2016.csv', 'posts2017.csv', 'posts2018.csv', 'posts2019.csv', 'posts2020.csv']\n",
    "USE_STOPWORDS = False\n",
    "\n",
    "if not USE_STOPWORDS:\n",
    "    CAPTION_PATH = DATA_PATH + 'caption_full/'\n",
    "    LEM_CAPTION_PATH = DATA_PATH + 'lem_caption_full/'\n",
    "\n",
    "print('success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gec_EjMzEXcg",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished posts2016.csv\n",
      "finished posts2017.csv\n",
      "finished posts2018.csv\n",
      "finished posts2019.csv\n",
      "finished posts2020.csv\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from gensim.parsing.preprocessing import strip_multiple_whitespaces\n",
    "from gensim.parsing.preprocessing import strip_non_alphanum\n",
    "\n",
    "MIN_DOCUMENT_SIZE = 2\n",
    "HASH_DELIMITER = 'ЙWЙ'\n",
    "\n",
    "WITH_STOPWORDS = False\n",
    "\n",
    "useless_symbols = set(['_', 'ー'])\n",
    "\n",
    "filter_words = set()\n",
    "if USE_STOPWORDS:\n",
    "    # load stopwords from nltk\n",
    "    filter_words = filter_words.union(set(stopwords.words('english')))\n",
    "    filter_words = filter_words.union(set(stopwords.words('russian')))\n",
    "    # load own stopwords\n",
    "    with open('stopwords_full.txt') as f:\n",
    "        filter_words = filter_words.union(set([word for line in f for word in line.split()]))\n",
    "\n",
    "\n",
    "# input: document: str\n",
    "# output: document: str\n",
    "# make preprocessing of documents, remove useless symbols, remove useless words\n",
    "def preprocessor(s: str):\n",
    "    # all letter to lower case\n",
    "    s = s.lower() \n",
    "    \n",
    "    # save '#' symbol\n",
    "    # s = s.replace('#', HASH_DELIMITER)\n",
    "    \n",
    "    # remove all symbol, which aren't letter or num\n",
    "    s = strip_non_alphanum(s)\n",
    "    # strip_non_alphanum don't remove '_' symbol\n",
    "    s = ''.join(map(lambda c: ' ' if c in useless_symbols else c, s))\n",
    "\n",
    "    # restore '#' symbol\n",
    "    # s = s.replace(HASH_DELIMITER, ' #')\n",
    "  \n",
    "    #s = ''.join(my_stem.lemmatize(s))\n",
    "    # replace all space symbol like space, tab, \\n to simple space and remove double and for space\n",
    "    s = strip_multiple_whitespaces(s)\n",
    "    # remove empty words and words from filtered words \n",
    "    s = ' '.join(list(filter(lambda w:  len(w) > 0 and not (w in filter_words or w.isdigit()), s.split(' '))))\n",
    "    return s\n",
    "\n",
    "\n",
    "for post_file in CAPTIONS:\n",
    "    # load file and drop documents with empty text\n",
    "    df = pd.read_csv(ROW_CAPTION_PATH + post_file).dropna()\n",
    "    # preprocess of corpus\n",
    "    df['caption'] = df['caption'].apply(preprocessor)\n",
    "    # remove empty documents\n",
    "    df = df[df['caption'].map(len) > MIN_DOCUMENT_SIZE]\n",
    "    # save results\n",
    "    df.to_csv(r'' + CAPTION_PATH + post_file, index=False)\n",
    "    print(\"finished \" + post_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ClNISbTos-PL"
   },
   "source": [
    "Вместо леммитицазии всех докуентов поотдельности можно обработать весь текст, после составить словарь всех слов, леммитизировать все по одному разу и перезаписать текст постов\n",
    "\n",
    "* pymystem3 - not very fast\n",
    "* spacy - need to check\n",
    "* pymorphy2 - slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1803670,
     "status": "ok",
     "timestamp": 1586265073616,
     "user": {
      "displayName": "Михаил Ковальчук",
      "photoUrl": "",
      "userId": "00328138544128199672"
     },
     "user_tz": -180
    },
    "id": "bUg3MpFVfaGG",
    "outputId": "76049a06-47d7-426b-c8f8-8cce57a0e4c9",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load words from posts2016.csv\n",
      "load words from posts2017.csv\n",
      "load words from posts2018.csv\n",
      "load words from posts2019.csv\n",
      "load words from posts2020.csv\n",
      "repleced words in posts2016.csv\n",
      "repleced words in posts2017.csv\n",
      "repleced words in posts2018.csv\n",
      "repleced words in posts2019.csv\n",
      "repleced words in posts2020.csv\n"
     ]
    }
   ],
   "source": [
    "# fast lemmatize\n",
    "from collections import defaultdict\n",
    "\n",
    "d = defaultdict(int)\n",
    "\n",
    "# load all words to dictionary\n",
    "for i in range(len(CAPTIONS)):\n",
    "    df = pd.read_csv(CAPTION_PATH + CAPTIONS[i])\n",
    "\n",
    "    for text in df['caption']:\n",
    "        words = text.split(' ')\n",
    "        for word in words:\n",
    "            d[word] += 1\n",
    "    \n",
    "    print('load words from ' + CAPTIONS[i])\n",
    "\n",
    "\n",
    "# create lemmatized dictionary\n",
    "d_lem = { word: ''.join(my_stem.lemmatize(word)[:-1]) for word in d } # [:-1] for remove \\n after lemmatize\n",
    "\n",
    "# save dictionary \n",
    "f = open(DATA_PATH + 'dictionary.txt', 'w')\n",
    "for word in d:\n",
    "    f.write(word + ',' + d_lem[word] + ',' + str(d[word]) + '\\n')\n",
    "f.close()\n",
    "\n",
    "# replece all words in dataset to lemmatized words\n",
    "for i in range(len(CAPTIONS)):\n",
    "    df = pd.read_csv(CAPTION_PATH + CAPTIONS[i])\n",
    "\n",
    "    df['caption'] = df['caption'].apply(lambda caption: ' '.join(list(map(lambda word: d_lem[word], caption.split(' ')))))\n",
    "\n",
    "    df.to_csv(r'' + LEM_CAPTION_PATH + CAPTIONS[i], index=False)\n",
    "    print(\"repleced words in \" + CAPTIONS[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 30814,
     "status": "ok",
     "timestamp": 1586267032890,
     "user": {
      "displayName": "Михаил Ковальчук",
      "photoUrl": "",
      "userId": "00328138544128199672"
     },
     "user_tz": -180
    },
    "id": "F-NJgdVAw9GK",
    "outputId": "ce50ad29-9f62-4604-b688-99026a1eb366",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "with open(DATA_PATH + 'dictionary.txt') as f:\n",
    "    d_list = [[token for token in line.split(',')] for line in f]\n",
    "\n",
    "\n",
    "d_lem = defaultdict(int)\n",
    "\n",
    "\n",
    "for note in d_list:\n",
    "    d_lem[note[1]] += int(note[2])\n",
    "\n",
    "l = list(d_lem.items())   \n",
    "l.sort(reverse=True, key=lambda item: item[1])\n",
    "\n",
    "print(len(l))\n",
    "l = list(filter(lambda item: item[1] > 1, l))\n",
    "print(len(l))\n",
    "\n",
    "for i in range(500):\n",
    "    print(i, l[i])\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMZsRMlgE+83bZ7K7jNXAb5",
   "machine_shape": "hm",
   "name": "filter_caption.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
